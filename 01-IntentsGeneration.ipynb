{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of Intents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@115904 We'll be sure to pass along your kind ...</td>\n",
       "      <td>@AmericanAir Erica on the lax team is amazing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@115904 Our apologies for the delay in respond...</td>\n",
       "      <td>@AmericanAir Could you have someone on your la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@115905 Aww, that's definitely a future pilot ...</td>\n",
       "      <td>Ben Tennyson and an American Airlines pilot. ðŸŽƒ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@115906 We're sorry for your frustration.</td>\n",
       "      <td>@AmericanAir Right, but I earned those. I also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@115909 We're glad you got to kick back and en...</td>\n",
       "      <td>Thank you, @AmericanAir for playing #ThisIsUs ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            responce  \\\n",
       "0  @115904 We'll be sure to pass along your kind ...   \n",
       "1  @115904 Our apologies for the delay in respond...   \n",
       "2  @115905 Aww, that's definitely a future pilot ...   \n",
       "3          @115906 We're sorry for your frustration.   \n",
       "4  @115909 We're glad you got to kick back and en...   \n",
       "\n",
       "                                            question  \n",
       "0  @AmericanAir Erica on the lax team is amazing ...  \n",
       "1  @AmericanAir Could you have someone on your la...  \n",
       "2  Ben Tennyson and an American Airlines pilot. ðŸŽƒ...  \n",
       "3  @AmericanAir Right, but I earned those. I also...  \n",
       "4  Thank you, @AmericanAir for playing #ThisIsUs ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"question_responce.csv\").drop(columns='Unnamed: 0')\n",
    "data = data.drop_duplicates()\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing \n",
    "\n",
    "**Steps :**\n",
    "\n",
    "- Tokenization: Split the text into sentences and the sentences into words. Lowercase the words and remove punctuation.\n",
    "- Words that have fewer than 3 characters are removed.\n",
    "- All stopwords are removed.\n",
    "- All words starting by '@' and '#' are removed\n",
    "- Words are lemmatized â€” words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "- Words are stemmed â€” words are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mathy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mathy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\mathy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "np.random.seed(2018)\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n",
    "    text = \" \".join(filter(lambda x:x[0]!='#', text.split()))\n",
    "    text = \" \".join(filter(lambda x:x[0:4]!='http', text.split()))\n",
    "    text = \" \".join(w for w in nltk.wordpunct_tokenize(text) \\\n",
    "        if w.lower() in words or not w.isalpha())\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                      [team, amaz, rais]\n",
       "1                               [team, avail, guid, gate]\n",
       "2                                                 [pilot]\n",
       "3              [right, pass, spous, need, chang, program]\n",
       "4                    [thank, great, flight, flight, home]\n",
       "                              ...                        \n",
       "1847       [transatlant, servic, abl, join, daili, scrum]\n",
       "1848                              [averag, price, ticket]\n",
       "1849                                 [month, claim, tell]\n",
       "1850    [terribl, servic, wait, tri, number, go, respons]\n",
       "1851                               [chang, time, airport]\n",
       "Name: question_prepro, Length: 1852, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['question_prepro'] = data['question'].apply(preprocess)\n",
    "data['question_prepro']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to clean the responce too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_preprocess(text):\n",
    "    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n",
    "    text = \" \".join(filter(lambda x:x[0]!='#', text.split()))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            We'll be sure to pass along your kind words!\n",
       "1       Our apologies for the delay in responding to y...\n",
       "2       Aww, that's definitely a future pilot in the m...\n",
       "3                       We're sorry for your frustration.\n",
       "4       We're glad you got to kick back and enjoy a sh...\n",
       "                              ...                        \n",
       "1847    We know staying connected is important, why no...\n",
       "1848    We've capped our fares for nonstop flights at ...\n",
       "1849    Please give our Baggage team a call at 800-866...\n",
       "1850    Our apologies for the hold. Our Central Baggag...\n",
       "1851    We're providing waivers for St Croix, Gillian....\n",
       "Name: responce, Length: 1852, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['responce'] = data['responce'].apply(response_preprocess)\n",
    "data['responce']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop empty questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>question_prepro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We'll be sure to pass along your kind words!</td>\n",
       "      <td>@AmericanAir Erica on the lax team is amazing ...</td>\n",
       "      <td>[team, amaz, rais]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our apologies for the delay in responding to y...</td>\n",
       "      <td>@AmericanAir Could you have someone on your la...</td>\n",
       "      <td>[team, avail, guid, gate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We're sorry for your frustration.</td>\n",
       "      <td>@AmericanAir Right, but I earned those. I also...</td>\n",
       "      <td>[right, pass, spous, need, chang, program]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We're glad you got to kick back and enjoy a sh...</td>\n",
       "      <td>Thank you, @AmericanAir for playing #ThisIsUs ...</td>\n",
       "      <td>[thank, great, flight, flight, home]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>We never want your experience to be anything l...</td>\n",
       "      <td>@AmericanAir's wifi makes Amtrak's wifi look p...</td>\n",
       "      <td>[look, their, free, decent, reason]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>We know staying connected is important, why no...</td>\n",
       "      <td>@AmericanAir and @172 have nailed in the trans...</td>\n",
       "      <td>[transatlant, servic, abl, join, daili, scrum]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>We've capped our fares for nonstop flights at ...</td>\n",
       "      <td>@AmericanAir Average price of ticket out: $250...</td>\n",
       "      <td>[averag, price, ticket]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>Please give our Baggage team a call at 800-866...</td>\n",
       "      <td>@AmericanAir Really annoyed been over a month ...</td>\n",
       "      <td>[month, claim, tell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>Our apologies for the hold. Our Central Baggag...</td>\n",
       "      <td>@AmericanAir terrible service wait ages trying...</td>\n",
       "      <td>[terribl, servic, wait, tri, number, go, respons]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>We're providing waivers for St Croix, Gillian....</td>\n",
       "      <td>@AmericanAir charges their patrons to change t...</td>\n",
       "      <td>[chang, time, airport]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1669 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               responce  \\\n",
       "0          We'll be sure to pass along your kind words!   \n",
       "1     Our apologies for the delay in responding to y...   \n",
       "3                     We're sorry for your frustration.   \n",
       "4     We're glad you got to kick back and enjoy a sh...   \n",
       "5     We never want your experience to be anything l...   \n",
       "...                                                 ...   \n",
       "1847  We know staying connected is important, why no...   \n",
       "1848  We've capped our fares for nonstop flights at ...   \n",
       "1849  Please give our Baggage team a call at 800-866...   \n",
       "1850  Our apologies for the hold. Our Central Baggag...   \n",
       "1851  We're providing waivers for St Croix, Gillian....   \n",
       "\n",
       "                                               question  \\\n",
       "0     @AmericanAir Erica on the lax team is amazing ...   \n",
       "1     @AmericanAir Could you have someone on your la...   \n",
       "3     @AmericanAir Right, but I earned those. I also...   \n",
       "4     Thank you, @AmericanAir for playing #ThisIsUs ...   \n",
       "5     @AmericanAir's wifi makes Amtrak's wifi look p...   \n",
       "...                                                 ...   \n",
       "1847  @AmericanAir and @172 have nailed in the trans...   \n",
       "1848  @AmericanAir Average price of ticket out: $250...   \n",
       "1849  @AmericanAir Really annoyed been over a month ...   \n",
       "1850  @AmericanAir terrible service wait ages trying...   \n",
       "1851  @AmericanAir charges their patrons to change t...   \n",
       "\n",
       "                                        question_prepro  \n",
       "0                                    [team, amaz, rais]  \n",
       "1                             [team, avail, guid, gate]  \n",
       "3            [right, pass, spous, need, chang, program]  \n",
       "4                  [thank, great, flight, flight, home]  \n",
       "5                   [look, their, free, decent, reason]  \n",
       "...                                                 ...  \n",
       "1847     [transatlant, servic, abl, join, daili, scrum]  \n",
       "1848                            [averag, price, ticket]  \n",
       "1849                               [month, claim, tell]  \n",
       "1850  [terribl, servic, wait, tri, number, go, respons]  \n",
       "1851                             [chang, time, airport]  \n",
       "\n",
       "[1669 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data['question_prepro'].map(lambda d: len(d)) > 1]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data['question_prepro'])\n",
    "id2word.filter_extremes(no_below=5)\n",
    "corpus = [id2word.doc2bow(text) for text in data['question_prepro']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 0.67036855)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model[corpus[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try to extract topic from new text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_highest_topic(text):\n",
    "  text_cleaned = preprocess(text)\n",
    "  result = lda_model[id2word.doc2bow(text_cleaned)]\n",
    "  #print(\"Max topic topic : \", max(result, key=itemgetter(1))[0], \n",
    "  #     \"\\nWith : \", max(result, key=itemgetter(1))[1])\n",
    "  return max(result, key=itemgetter(1))[0]\n",
    "\n",
    "get_highest_topic('Ben Tennyson  Airlines pilot.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "      <th>question_prepro</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We'll be sure to pass along your kind words!</td>\n",
       "      <td>@AmericanAir Erica on the lax team is amazing ...</td>\n",
       "      <td>[team, amaz, rais]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Our apologies for the delay in responding to y...</td>\n",
       "      <td>@AmericanAir Could you have someone on your la...</td>\n",
       "      <td>[team, avail, guid, gate]</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We're sorry for your frustration.</td>\n",
       "      <td>@AmericanAir Right, but I earned those. I also...</td>\n",
       "      <td>[right, pass, spous, need, chang, program]</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We're glad you got to kick back and enjoy a sh...</td>\n",
       "      <td>Thank you, @AmericanAir for playing #ThisIsUs ...</td>\n",
       "      <td>[thank, great, flight, flight, home]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>We never want your experience to be anything l...</td>\n",
       "      <td>@AmericanAir's wifi makes Amtrak's wifi look p...</td>\n",
       "      <td>[look, their, free, decent, reason]</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            responce  \\\n",
       "0       We'll be sure to pass along your kind words!   \n",
       "1  Our apologies for the delay in responding to y...   \n",
       "3                  We're sorry for your frustration.   \n",
       "4  We're glad you got to kick back and enjoy a sh...   \n",
       "5  We never want your experience to be anything l...   \n",
       "\n",
       "                                            question  \\\n",
       "0  @AmericanAir Erica on the lax team is amazing ...   \n",
       "1  @AmericanAir Could you have someone on your la...   \n",
       "3  @AmericanAir Right, but I earned those. I also...   \n",
       "4  Thank you, @AmericanAir for playing #ThisIsUs ...   \n",
       "5  @AmericanAir's wifi makes Amtrak's wifi look p...   \n",
       "\n",
       "                              question_prepro  topic  \n",
       "0                          [team, amaz, rais]     10  \n",
       "1                   [team, avail, guid, gate]     69  \n",
       "3  [right, pass, spous, need, chang, program]     21  \n",
       "4        [thank, great, flight, flight, home]      4  \n",
       "5         [look, their, free, decent, reason]     88  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['topic'] = data['question'].apply(get_highest_topic)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = {\"Greeting\": [\"Hello\", \"How are you doing?\", \"Greetings!\", \"How do you do?\"],\n",
    "          \"Apology\": [\"No problem\"],\n",
    "          \"Thanks\": [\"No problem\", \"You're welcome\"],\n",
    "          \"Goodbye\": [\"It was nice speaking to you\", \"See you later\", \"Speak soon!\"]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_intents(df):\n",
    "  for index, row in df.iterrows():\n",
    "    if str(row['topic']) not in intents :\n",
    "      intents[str(row['topic'])] = []\n",
    "    if row['responce'] != \"\":\n",
    "      intents[str(row['topic'])].append(str(row['responce']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_intents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Intents in order to use it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json', 'w') as fp:\n",
    "    json.dump(intents, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(data['responce'])]\n",
    "model = Doc2Vec(tagged_data, window=1, min_count=2, epochs = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(text):\n",
    "  result = model.docvecs.most_similar(positive=[model.infer_vector(preprocess(text))], topn=1800)\n",
    "  topic = get_highest_topic('text')\n",
    "  for l in result:\n",
    "    if data['topic'].iloc[int(l[0])] == topic:\n",
    "      return data['responce'].iloc[int(l[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : Enjoying the view?\n",
      "Prediction : Please follow and meet us in DMs with your record locator and we'll be happy to take a peek.\n",
      "Prediction : Purchasing Main Cabin Extra or Preferred seats is an option offered in advance. Seats will be assigned once you arrive to the airport!\n",
      "Prediction : Please follow and DM your record locator, Thomas. We'd like let our App Team know.\n",
      "Expected : Looks like it has a maintenance delay and it's currently scheduled to take off at 10:10p.\n",
      "Expected : We do many checks pre and post departure. Please share your flight number if we can provide an update.\n",
      "Expected : We don't tolerate discrimination of any kind. Please DM your record locator and contact details (phone and email)\n",
      "Expected : We'd like to share this feedback. Please DM your record locator.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathy\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "list_word = [\"@AmericanAir whatâ€™s going on with flight 301 DTW &gt; DFW? Delayed over 3 hours. Will it be canceled?\", \n",
    "             \"@AmericanAir awful service, more than 1 hour delay due to logbook maintenance issues. Where is preventive maintenance?\",\n",
    "             \"@AmericanAir Yes. Refund my plane ticketðŸ˜¡ with your racist ass workers\",\n",
    "             \"@AmericanAir I'm aware. Fits in overhead.This was a return trip. Had same luggage going. This was pre-security. Gate chck, ok, but Empty overheads on flt\"]\n",
    "\n",
    "list_resp = [\"Looks like it has a maintenance delay and it's currently scheduled to take off at 10:10p.\",\n",
    "             \"We do many checks pre and post departure. Please share your flight number if we can provide an update.\",\n",
    "             \"We don't tolerate discrimination of any kind. Please DM your record locator and contact details (phone and email)\",\n",
    "             \"We'd like to share this feedback. Please DM your record locator.\"]\n",
    "\n",
    "for q in list_word:\n",
    "  print(\"Prediction :\", chatbot(q))\n",
    "\n",
    "for r in list_resp:\n",
    "  print(\"Expected :\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathy\\Anaconda3\\envs\\NLP\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Fabulous indeed!'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input('Enter your question : ')\n",
    "chatbot(question)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b80020ff525a21f93a46f2383b1e2d9149a932cd9ca76a8e8231a40d28a5b620"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
